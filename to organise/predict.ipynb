{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=load_model(\"models/GenderModel.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file_path = '/Users/dheemankumar/github/audio-ai/ab.wav'\n",
    "\n",
    "audio, sample_rate = librosa.load(audio_file_path, sr=22050)  # Load audio with a sample rate of 22050\n",
    "\n",
    "d=librosa.stft(audio)\n",
    "\n",
    "s_db=librosa.amplitude_to_db(np.abs(d),ref=np.max)\n",
    "s_db_with_channel = np.expand_dims(s_db, axis=-1)\n",
    "\n",
    "audio_= np.array(s_db_with_channel)\n",
    "\n",
    "input_data = audio_.reshape(1, 1025, 130, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_file_path2 = '/Users/dheemankumar/github/audio-ai/female_eng.wav'  # Adjust the path as needed\n",
    "audio2, sample_rate = librosa.load(audio_file_path2, sr=22050)  # Load audio with a sample rate of 22050\n",
    "\n",
    "d2=librosa.stft(audio2)\n",
    "\n",
    "s_db2=librosa.amplitude_to_db(np.abs(d2),ref=np.max)\n",
    "s_db_with_channel2 = np.expand_dims(s_db2, axis=-1)\n",
    "\n",
    "audio_= np.array(s_db_with_channel2)\n",
    "\n",
    "input_data2 = audio_.reshape(1, 1025, 130, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise_predictions = np.argmax(model_noise.predict(input_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 162ms/step\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# List of audio file paths\n",
    "audio_file_paths = ['/Users/dheemankumar/github/audio-ai/ad.wav', '/Users/dheemankumar/github/audio-ai/ab.wav', '/Users/dheemankumar/github/audio-ai/female_eng.wav']  # Add your file paths\n",
    "\n",
    "# Initialize an empty list to store predictions\n",
    "predictions_list = []\n",
    "\n",
    "# Loop over each audio file\n",
    "for audio_file_path in audio_file_paths:\n",
    "    # Load audio with a sample rate of 22050\n",
    "    audio, sample_rate = librosa.load(audio_file_path, sr=22050)\n",
    "\n",
    "    # Compute spectrogram\n",
    "    d = librosa.stft(audio)\n",
    "    s_db = librosa.amplitude_to_db(np.abs(d), ref=np.max)\n",
    "    s_db_with_channel = np.expand_dims(s_db, axis=-1)\n",
    "\n",
    "    # Reshape for model input\n",
    "    input_data = s_db_with_channel.reshape(1, 1025, 130, 1)\n",
    "\n",
    "    # Append input data to the list\n",
    "    predictions_list.append(input_data)\n",
    "\n",
    "# Stack the input data along the first axis to create a single array\n",
    "all_input_data = np.vstack(predictions_list)\n",
    "\n",
    "\n",
    "# Make predictions for all input data at once\n",
    "all_predictions = model.predict(all_input_data)\n",
    "\n",
    "# Assuming a classification task with one-hot encoded labels, convert predictions to class labels\n",
    "predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "\n",
    "# Now 'predicted_classes' contains the predicted class labels for each audio file\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1025, 130, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
